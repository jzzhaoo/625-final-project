---
title: "Biostat 625 Final Project draft2"
author: "Haisheng Xu (haisheng)"
date: "12/13/2021"
output:
  pdf_document: default
  html_document: default
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Data Cleaning
```{r d}

data2 = read.csv("Liquor_Items.csv")
data3 = na.omit(data2)

data_cleaner = data3[data3$Bottles.Sold>=730,]
data_cleaner$Category = as.factor(data_cleaner$Category)
data_cleaner$Vendor.Number = as.factor(data_cleaner$Vendor.Number)


data_index = read.csv("Covid_index.csv")


data_index = merge(x = data_cleaner, y = data_index, by = "Item.Number", all = TRUE)
data_index = (na.omit(data_index))
data_index = data_index[-12]
data_index$PopularityC = ifelse(data_index$Covid_index >= 0, 1, 0)




data_p = data_index[,-c(10,12,13)]
data_p$Popularity = ifelse(data_p$Popularity == "Popular", 1, 0)

data_c = data_index[,-c(11,12)]
```
# Data Correlation
```{r c1}
library(corrplot)
#data3$Store.Number

M <- cor(data3[,c(4:10)])

# corrplot(M, method="color")

col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(M, method="color", col=col(200),  
         type="upper",  
         addCoef.col = "black", # Add coefficient of correlation
         tl.col="black", tl.srt=45, #Text label color and rotation
         # Combine with significance
          insig = "blank", 
         # hide correlation coefficient on the principal diagonal
         diag=FALSE 
         )

```



```{r classprep}
library(caTools)
data_used = data_p
set.seed(111111)
split = sample.split(data_used$Popularity, SplitRatio = 0.60)
 
training_set = subset(data_used, split == TRUE)
test_set = subset(data_used, split == FALSE)


data_used = data_c
set.seed(111111)
split = sample.split(data_used$Popularity, SplitRatio = 0.60)
 
training_setc = subset(data_used, split == TRUE)
test_setc = subset(data_used, split == FALSE)


```



```{r glm1}
library(e1071)
library(caret)
library(ResourceSelection)
library(ggplot2)


full = glm(Popularity~.-Item.Number, data = training_set, family=binomial)
nullmodel = glm(Popularity~1, data = training_set, family=binomial)
n=nrow(training_set)
fit_step = step(nullmodel,scope=list(lower= nullmodel,
upper=full),direction="both",k=log(n))

glm1 = summary(fit_step)$coefficients
hoslem.test(fit_step$y, fit_step$fitted.values,g=10)



etahat_fit = predict(fit_step, type = "link")
pb_fit = predict(fit_step, type = "response")

ggplot(training_set,aes(x= etahat_fit,y= pb_fit))+
geom_point(aes(color=factor(Popularity)),position=position_jitter(height=0.03,width=0),size=0.5)+
geom_line(aes(x= etahat_fit,y=pb_fit))+
labs(x="eta_hat",y="probability")+
scale_color_manual(values=c("red","blue"),name="Popularity",labels=c("Popular","Unpopular"))+
geom_hline(yintercept=0.49,linetype="dashed")+
geom_vline(xintercept=-0.15,linetype="dashed")+
scale_y_continuous(breaks=seq(0,1,by=0.1))+theme_bw()



pihat_test = predict(fit_step,newdata=test_set,
type="response")
threshold = 0.49
predicted_category =
factor(ifelse(pihat_test>threshold, 1,0) )
cm11 = confusionMatrix(data= predicted_category,reference= as.factor(as.numeric(test_set$Popularity)))
cm11
```
```{r glm2}

full = glm(PopularityC~.-Item.Number, data = training_setc, family=binomial)
nullmodel = glm(PopularityC~1, data = training_setc, family=binomial)
n=nrow(training_setc)
fit_step = step(nullmodel,scope=list(lower= nullmodel,
upper=full),direction="both",k=log(n))

glm2 = summary(fit_step)$coefficients

#Only one variable significant: Retailed

#hoslem.test(fit_step$y, fit_step$fitted.values,g=10)


# 
# etahat_fit = predict(fit_step, type = "link")
# pb_fit = predict(fit_step, type = "response")
# 
# ggplot(training_setc,aes(x= etahat_fit,y= pb_fit))+
# geom_point(aes(color=factor(PopularityC)),position=position_jitter(height=0.03,width=0),size=0.5)+
# geom_line(aes(x= etahat_fit,y=pb_fit))+
# labs(x="eta_hat",y="probability")+
# scale_color_manual(values=c("red","blue"),name="PopularityC",labels=c("Popular","Unpopular"))+
# geom_hline(yintercept=0.49,linetype="dashed")+
# geom_vline(xintercept=-0.15,linetype="dashed")+
# scale_y_continuous(breaks=seq(0,1,by=0.1))+theme_bw()
# 
# #
# #
# pihat_test = predict(fit_step,newdata=test_setc,
# type="response")
# threshold = 0.49
# predicted_category =
# factor(ifelse(pihat_test>threshold, 1,0) )
# confusionMatrix(data= predicted_category,reference= as.factor(as.numeric(test_set$Popularity)))


```
```{r tree1}

#Random Forest Cannot more than 53 levels

# Decision Tree
library(rpart)
library(rpart.plot)
library(dplyr)
library(ggplot2)

fit <- rpart(Popularity~.-Item.Number, data = training_set, method = 'class')
rpart.plot(fit,cex = 0.5)

predict_unseen <-predict(fit, test_set, type = 'class')
table_mat <- table(test_set$Popularity, predict_unseen)
cm12 = confusionMatrix(table_mat)
cm12

# Importance of variables
importance = data.frame(imp = fit$variable.importance)
df2 <- importance %>% 
  tibble::rownames_to_column() %>% 
  dplyr::rename("variable" = rowname) %>% 
  dplyr::arrange(imp) %>%
  dplyr::mutate(variable = forcats::fct_inorder(variable))

ggplot2::ggplot(df2) +
  geom_col(aes(x = variable, y = imp),
           col = "black", show.legend = F) +
  coord_flip() +
  scale_fill_grey() +
  theme_bw() + ggtitle("Importance of Variable for Decision Tree (General Popularity)")

```

```{r tree2}

#Random Forest Cannot more than 53 levels

# Decision Tree

fit <- rpart(PopularityC~.-Item.Number, data = training_setc, method = 'class')
rpart.plot(fit,cex = 0.5)

predict_unseen <-predict(fit, test_setc, type = 'class')
table_mat <- table(test_setc$PopularityC, predict_unseen)
cm22 = confusionMatrix(table_mat)
cm22

# Importance of variables
importance = data.frame(imp = fit$variable.importance)
df2 <- importance %>% 
  tibble::rownames_to_column() %>% 
  dplyr::rename("variable" = rowname) %>% 
  dplyr::arrange(imp) %>%
  dplyr::mutate(variable = forcats::fct_inorder(variable))

ggplot2::ggplot(df2) +
  geom_col(aes(x = variable, y = imp),
           col = "black", show.legend = F) +
  coord_flip() +
  scale_fill_grey() +
  theme_bw()

```



```{r nn}
# too many levels
```


```{r svm}
library(caTools)
library(e1071)
library(caret)

classifier = svm(formula = Popularity~.-Item.Number,
                 data = training_set,
                 type = 'C-classification',
                 kernel = 'linear')

summary(classifier)
y_pred = predict(classifier, newdata = test_set[,-10])
cm = table(test_set$Popularity, y_pred)
cm13 = confusionMatrix(cm)
cm13

# Cannot plot for more than 2 predictors
```
```{r svm2}
library(caTools)
library(e1071)
library(caret)

classifier = svm(formula = PopularityC~.-Item.Number,
                 data = training_setc,
                 type = 'C-classification',
                 kernel = 'linear')


y_pred = predict(classifier, newdata = test_setc[,-11])
cm = table(test_setc$PopularityC, y_pred)
cm23 = confusionMatrix(cm)
cm23

# Cannot plot for more than 2 predictors
```
```{r knn}
library(e1071)
library(caTools)
library(class)

n = dim(training_set)[1]
set.seed(1111)
# Using the elbow method to decide the number of clusters.
K.list = 2:13
cost= rep(NA, length(K.list))
for (i in 1:length(K.list)){
  K.i = K.list[i]
  mu.i = training_set[sample(1:n, size=K.i, replace = FALSE), ]
  km.i <- kmeans(training_set, centers=mu.i)
  cost[i] = km.i$tot.withinss
}
# Plot the elbow curve
plot(K.list, cost, type='b')


classifier_knn <- knn(train = training_set[,-1],
                      test = test_set[,-1],
                      cl = training_set$Popularity,
                      k = 5)
#classifier_knn
cm <- table(test_set$Popularity, classifier_knn)
cm

misClassError <- mean(classifier_knn != test_set$Popularity)
print(paste('Accuracy =', 1-misClassError))

cm14 = confusionMatrix(cm)
cm14
```
```{r knn2}
n = dim(training_setc)[1]
set.seed(1111)
# Using the elbow method to decide the number of clusters.
K.list = 2:13
cost= rep(NA, length(K.list))
for (i in 1:length(K.list)){
  K.i = K.list[i]
  mu.i = training_setc[sample(1:n, size=K.i, replace = FALSE), ]
  km.i <- kmeans(training_setc, centers=mu.i)
  cost[i] = km.i$tot.withinss
}
# Plot the elbow curve
plot(K.list, cost, type='b')


classifier_knn <- knn(train = training_setc[,-1],
                      test = test_setc[,-1],
                      cl = training_setc$PopularityC,
                      k = 6)
#classifier_knn
cm <- table(test_setc$PopularityC, classifier_knn)
cm

misClassError <- mean(classifier_knn != test_set$Popularity)
print(paste('Accuracy =', 1-misClassError))

cm24 = confusionMatrix(cm)
cm24
```


```{r lasso}
library(tidyverse)
library(caret)
library(glmnet)


cv.lasso = cv.glmnet(model.matrix(Popularity~.-Item.Number-Vendor.Number, training_set)[,-1], training_set$Popularity, family = "binomial", alpha = 1)

lasso = glmnet(model.matrix(Popularity~.-Item.Number-Vendor.Number, training_set)[,-1], training_set$Popularity, family = "binomial", alpha = 1, lambda = cv.lasso$lambda.min)

las1 = coef(lasso)


x.test <- model.matrix(Popularity~.-Item.Number-Vendor.Number, test_set)[,-1]
probabilities <- lasso %>% predict(newx = x.test)
predicted.classes <- ifelse(probabilities > 0.5, 1, 0)

# Model accuracy
observed.classes <- test_set$Popularity
mean(predicted.classes == observed.classes)
cm = table(predicted.classes, observed.classes)
cm15 = confusionMatrix(cm)
cm15
```

```{r lasso2}
cv.lasso = cv.glmnet(model.matrix(PopularityC~.-Item.Number, training_setc)[,-1], training_setc$PopularityC, family = "binomial", alpha = 1)

lasso = glmnet(model.matrix(PopularityC~.-Item.Number, training_setc)[,-1], training_setc$PopularityC, family = "binomial", alpha = 1, lambda = cv.lasso$lambda.min)

las2 = coef(lasso)


x.test <- model.matrix(PopularityC~.-Item.Number, test_setc)[,-1]
probabilities <- lasso %>% predict(newx = x.test)
predicted.classes <- ifelse(probabilities > 0.5, 1, 0)

# Model accuracy
observed.classes <- test_setc$PopularityC
mean(predicted.classes == observed.classes)

cm = table(predicted.classes, observed.classes)
cm25 = confusionMatrix(cm)
cm25
```

# Time series

```{r timeseriesfinalmodule1}

# Without considering seasonality in a short time period
library(lubridate)
library(astsa)

ts <-read.csv("salesbyweek.csv")
ts.sales <-ts(ts[,2],start=decimal_date(ymd("2017-10-31")),freq=365.25/7)

tsplot(ts.sales,type="l")

diff.log <- diff(log(ts.sales))
tsplot(diff.log, xlab = "", ylab = "Log(Sales)", main="")

acf(diff.log)
pacf(diff.log)



#Interpretation 1: Maybe the ACF is cutting off at lag 1 and the PACF is tailing off. This would suggest an MA(1) for
#diff.log, which is equivalent to an ARIMA(0,1,1) for log.
#Interpretation 2: Maybe the ACF is tailing off and the PACF is cutting off at lag 7. This would suggest an AR(7) for diff.log.p,
# which is equivalent to an ARIMA(7,1,0) for log.p.
fit.ma1 <- sarima(diff.log,0,0,1)
fit.ar7 <- sarima(diff.log,7,0,0)

fit.ma1$AIC
fit.ma1$BIC
fit.ar7$AIC
fit.ar7$BIC
# Choose MA(1)

# Final model without considering the seasonality
ip.pred <- sarima.for(log(ts.sales),24,0,1,1)
exp(ip.pred$pred)
exp(ip.pred$pred-1.96*ip.pred$se)
exp(ip.pred$pred+1.96*ip.pred$se)
```


```{r timeseriesfinalmodule2}
library(lubridate)
library(astsa)
library(ggplot2)
library(forecast)

ts <-read.csv("salesbyweek.csv")
ts.sales <-ts(ts[,2],start=decimal_date(ymd("2017-10-31")),freq=365.25/7)


library(ggplot2)
p1 <- autoplot(ts.sales) +
  ylab("Sales") + xlab("Date")
p2 <- autoplot(window(ts.sales, end=2020)) +
  ylab("Sales") + xlab("Date") 
gridExtra::grid.arrange(p1,p2)


ts.sales %>% mstl() %>%
  autoplot() + xlab("Time")


#Seasonality
library(forecast)

# Automated forecasting using an ARIMA model
fit <- auto.arima(ts.sales) 
fit

#Complex seasonality
fit %>%
  forecast() %>%
  autoplot(include=208) +
    ylab("Sales") + xlab("Time")


#Complex seasonality
fit %>%
  forecast() %>%
  autoplot(include=208) +
    ylab("Sales") + xlab("Date")


forecast(fit)
```


```{r kablepart}
library(knitr)
library(kableExtra)


variablelist = c("General Popularity", "COVID Popularity")
methodname = c("GLM", "Decision Tree", "SVM", "KNN", "LASSO")

aclist = as.numeric(c(cm11$overall[1], cm12$overall[1], cm13$overall[1], cm14$overall[1], cm15$overall[1]))
aclist2 = as.numeric(c(NA, cm22$overall[1], cm23$overall[1], cm24$overall[1], cm25$overall[1]))

aclistb = as.numeric(c(cm11$byClass[11], cm12$byClass[11], cm13$byClass[11], cm14$byClass[11], cm15$byClass[11]))
aclistb2 = as.numeric(c(NA, cm22$byClass[11], cm23$byClass[11], cm24$byClass[11], cm25$byClass[11]))

msgnf = c()

overalldata1 = data.frame(aclist,aclistb)
overalldata2 = data.frame(aclist2, aclistb2)
row.names(overalldata1) = methodname
colnames(overalldata1) = c("Accuracy Rate","Balanced Accuracy Rate")
row.names(overalldata2) = methodname
colnames(overalldata2) = c("Accuracy Rate","Balanced Accuracy Rate")

  
overalldata1 %>%
  kbl(caption = "Classification for General Popularity") %>%
  kable_paper("hover", full_width = F)


overalldata2 %>%
  kbl(caption = "Classification for COVID Popularity") %>%
  kable_paper("hover", full_width = F)


glm1%>%
  kbl(caption = "GLM for General Popularity") %>%
  kable_paper("hover", full_width = F)

glm2%>%
  kbl(caption = "GLM for COVID Popularity") %>%
  kable_paper("hover", full_width = F)

```






